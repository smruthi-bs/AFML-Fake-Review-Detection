{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CELL 1 — Mount & load dataset, clean text, save review_index.csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/content/drive/MyDrive/afml_project\")\n",
        "OUT_DIR = ROOT / \"bert\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CSV_PATH = ROOT / \"TRAIN_model_ready_final_mode.csv\"   # change if needed\n",
        "# safe dtypes\n",
        "dtype_map = {\n",
        "    \"review_id\": \"string\",\n",
        "    \"user_id\": \"string\",\n",
        "    \"text\": \"string\",\n",
        "    \"gmap_id\": \"string\",\n",
        "    \"rating\": \"float64\",\n",
        "    \"datetime\": \"string\"\n",
        "}\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, dtype=dtype_map, low_memory=False)\n",
        "\n",
        "# ensure review_id exists as string\n",
        "if \"review_id\" not in df.columns or df[\"review_id\"].isna().all():\n",
        "    df[\"review_id\"] = df.index.astype(str)\n",
        "else:\n",
        "    df[\"review_id\"] = df[\"review_id\"].astype(\"string\")\n",
        "\n",
        "# parse datetime if present\n",
        "if \"datetime\" in df.columns:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
        "else:\n",
        "    df[\"datetime\"] = pd.NaT\n",
        "\n",
        "# minimal text cleaning\n",
        "def clean_text(s):\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    if isinstance(s, float) and pd.isna(s):\n",
        "        return \"\"\n",
        "    t = str(s).strip()\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "# save mapping (include datetime if present)\n",
        "cols = [\"review_id\", \"user_id\", \"gmap_id\", \"rating\", \"datetime\"]\n",
        "cols = [c for c in cols if c in df.columns]\n",
        "df_index = df[cols].copy()\n",
        "df_index.to_csv(OUT_DIR / \"review_index.csv\", index=False)\n",
        "\n",
        "print(\"Loaded rows:\", len(df))\n",
        "print(\"Saved review_index.csv to:\", OUT_DIR / \"review_index.csv\")"
      ],
      "metadata": {
        "id": "YsBbq7qY9V3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 — Compute SBERT embeddings (all-MiniLM-L6-v2) and save to embeddings.npy\n",
        "# If you already have model loaded in session, it's OK to run just this cell.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/afml_project/bert\")\n",
        "\n",
        "# load model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model.max_seq_length = 256\n",
        "\n",
        "# load texts from df or review_index file\n",
        "import pandas as pd\n",
        "df_index = pd.read_csv(OUT_DIR / \"review_index.csv\", dtype=str)\n",
        "# if you have original df in memory, you can use that instead; otherwise load text from the original CSV\n",
        "# Here we reload the original CSV to get text column (keeps cells independent)\n",
        "ROOT = Path(\"/content/drive/MyDrive/afml_project\")\n",
        "CSV_PATH = ROOT / \"TRAIN_model_ready_final_mode.csv\"\n",
        "df_full = pd.read_csv(CSV_PATH, dtype=str, low_memory=False)\n",
        "# Align by review_id to ensure same order\n",
        "df_full[\"review_id\"] = df_full[\"review_id\"].astype(str)\n",
        "df_index[\"review_id\"] = df_index[\"review_id\"].astype(str)\n",
        "merged = df_index.merge(df_full[[\"review_id\",\"text\"]], on=\"review_id\", how=\"left\")\n",
        "texts = merged[\"text\"].fillna(\"\").tolist()\n",
        "\n",
        "N = len(texts)\n",
        "batch_size = 512 if N <= 50000 else 256 if N <= 200000 else 128\n",
        "print(f\"Encoding {N} reviews with batch_size={batch_size} ...\")\n",
        "\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size=batch_size,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "np.save(OUT_DIR / \"embeddings.npy\", embeddings)\n",
        "print(\"Saved embeddings.npy with shape:\", embeddings.shape)"
      ],
      "metadata": {
        "id": "vWuiS-o89V52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 — Build clusters by gmap_id (deterministic: one cluster per business)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/afml_project/bert\")\n",
        "\n",
        "index_df = pd.read_csv(OUT_DIR / \"review_index.csv\", dtype=str).reset_index().rename(columns={\"index\":\"review_index\"})\n",
        "\n",
        "# Normalize gmap_id (treat blanks/NaN as missing)\n",
        "if \"gmap_id\" in index_df.columns:\n",
        "    index_df[\"gmap_id\"] = index_df[\"gmap_id\"].replace({\"\": None}).where(pd.notna(index_df[\"gmap_id\"]), None)\n",
        "    unique_gmaps = index_df[\"gmap_id\"].dropna().unique().tolist()\n",
        "    gmap_to_label = {g: i for i, g in enumerate(unique_gmaps)}\n",
        "    cluster_sizes = index_df.groupby(\"gmap_id\", dropna=False).size().to_dict()\n",
        "\n",
        "    cluster_labels = []\n",
        "    cluster_probs = []\n",
        "    cluster_size_list = []\n",
        "    for _, r in index_df.iterrows():\n",
        "        g = r.get(\"gmap_id\", None)\n",
        "        if pd.isna(g) or g is None:\n",
        "            cluster_labels.append(-1)\n",
        "            cluster_probs.append(0.0)\n",
        "            cluster_size_list.append(1)\n",
        "        else:\n",
        "            cluster_labels.append(int(gmap_to_label[g]))\n",
        "            cluster_probs.append(1.0)\n",
        "            cluster_size_list.append(int(cluster_sizes.get(g, 1)))\n",
        "else:\n",
        "    # no gmap_id column: mark all as outliers\n",
        "    cluster_labels = [-1] * len(index_df)\n",
        "    cluster_probs = [0.0] * len(index_df)\n",
        "    cluster_size_list = [1] * len(index_df)\n",
        "\n",
        "clusters_by_gmap = pd.DataFrame({\n",
        "    \"review_index\": index_df[\"review_index\"].astype(int),\n",
        "    \"cluster_label\": cluster_labels,\n",
        "    \"cluster_prob\": cluster_probs,\n",
        "    \"cluster_size\": cluster_size_list\n",
        "})\n",
        "\n",
        "clusters_by_gmap.to_csv(OUT_DIR / \"clusters_by_gmap.csv\", index=False)\n",
        "print(\"Saved clusters_by_gmap.csv to:\", OUT_DIR / \"clusters_by_gmap.csv\")\n",
        "print(\"Sample:\")\n",
        "print(clusters_by_gmap.head(8))"
      ],
      "metadata": {
        "id": "FLeePCLS9V9V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}