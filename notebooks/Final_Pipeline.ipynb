import os
import json
import random
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

import matplotlib.pyplot as plt
from sklearn.metrics import (accuracy_score, f1_score, classification_report,
                             roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve)
from sklearn.preprocessing import StandardScaler
import joblib

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.data import HeteroData
from torch_geometric.nn import RGCNConv

from transformers import AutoTokenizer, AutoModel
import xgboost as xgb

warnings.filterwarnings('ignore')


# =======================
# Configuration
# =======================
class Config:
    BASE_PATH = os.environ.get('AFML_BASE_PATH', '/content/drive/MyDrive/AFML_project/')
    # BERT_DIR was intended for organizing embeddings, but the previous script saved them directly in BASE_PATH.
    # We will adjust TRAIN_EMB and TEST_EMB to reflect this.
    # BERT_DIR = os.path.join(BASE_PATH := BASE_PATH, 'bert') # Removed as it's not directly used for TRAIN_EMB/TEST_EMB with this fix

    TRAIN_CSV = os.path.join(BASE_PATH, 'TRAIN_model_ready_final_mode.csv')
    TEST_CSV = os.path.join(BASE_PATH, 'TEST_model_ready_final_mode.csv')

    TRAIN_EMB = os.path.join(BASE_PATH, 'train_bert.npy') # Corrected path to match previous output
    TEST_EMB = os.path.join(BASE_PATH, 'test_bert.npy')   # Corrected path to match previous output

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    TEXT_COL = 'text'
    LABEL_COL = 'label'  # 1 = Fake, 0 = Real

    # Model / training
    bert_model = 'bert-base-uncased'  # used only if script must compute embedding for a single input
    xgb_n_estimators = 100
    rgcn_hidden_dim = 64
    num_epochs = 20
    learning_rate = 0.001

    out_dir = os.path.join(BASE_PATH, 'artifacts')


os.makedirs(Config.out_dir, exist_ok=True)


# =======================
# Helpers: load data and embeddings
# =======================

def load_and_prep_data():
    print(f"Loading data from {Config.BASE_PATH} ...")
    if not os.path.exists(Config.TRAIN_CSV):
        raise FileNotFoundError(f"Could not find {Config.TRAIN_CSV}")

    train_df = pd.read_csv(Config.TRAIN_CSV)
    test_df = pd.read_csv(Config.TEST_CSV)

    train_df[Config.TEXT_COL] = train_df[Config.TEXT_COL].fillna("")
    test_df[Config.TEXT_COL] = test_df[Config.TEXT_COL].fillna("")

    # If label missing, create dummy heuristic (only for demo)
    if Config.LABEL_COL not in train_df.columns:
        print("WARNING: label column missing. Creating heuristic labels from 'rating'.")
        train_df[Config.LABEL_COL] = train_df.get('rating', 0).apply(lambda x: 1 if x in [1, 5] else 0)
        test_df[Config.LABEL_COL] = test_df.get('rating', 0).apply(lambda x: 1 if x in [1, 5] else 0)

    # Ensure required cols
    for df in (train_df, test_df):
        for c in ['user_id', 'gmap_id', 'rating', 'datetime']:
            if c not in df.columns:
                df[c] = 0

    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)


def load_embeddings(preferred_path):
    if os.path.exists(preferred_path):
        print(f"Loading embeddings from {preferred_path}")
        return np.load(preferred_path)
    raise FileNotFoundError(f"Embeddings not found at {preferred_path}")


# =======================
# XGBoost user behavior model
# =======================

def process_xgb_features(train_df, test_df):
    print('\n--- Component 2: XGBoost Integration ---')

    def extract_user_feats(df):
        df = df.copy()
        # Safe parse
        try:
            df['datetime'] = pd.to_datetime(df['datetime'])
        except Exception:
            df['datetime'] = pd.to_datetime(df.get('datetime', pd.Timestamp.now()))

        grp = df.groupby('user_id')
        feats = grp.agg({
            'rating': ['mean', 'std', 'count'],
            'gmap_id': 'nunique'
        })
        feats.columns = ['rating_mean', 'rating_std', 'review_count', 'biz_diversity']
        feats = feats.reset_index()
        feats['rating_std'] = feats['rating_std'].fillna(0)
        return feats

    print("Building User Profiles...")
    train_profiles = extract_user_feats(train_df)
    test_profiles = extract_user_feats(test_df)

    # Merge back
    train_merged = train_df.merge(train_profiles, on='user_id', how='left')
    test_merged = test_df.merge(test_profiles, on='user_id', how='left')

    features = ['rating_mean', 'rating_std', 'review_count', 'biz_diversity']

    scaler = StandardScaler()
    X_train = scaler.fit_transform(train_merged[features].fillna(0).values)
    X_test = scaler.transform(test_merged[features].fillna(0).values)

    y_train = train_merged[Config.LABEL_COL].values

    print("Training XGBoost...")
    xgb_model = xgb.XGBClassifier(n_estimators=Config.xgb_n_estimators,
                                  max_depth=6, learning_rate=0.1,
                                  eval_metric='logloss', use_label_encoder=False)
    xgb_model.fit(X_train, y_train)

    train_xgb_prob = xgb_model.predict_proba(X_train)[:, 1]
    test_xgb_prob = xgb_model.predict_proba(X_test)[:, 1]

    train_merged['xgb_prob'] = train_xgb_prob
    test_merged['xgb_prob'] = test_xgb_prob

    # Save artifacts
    joblib.dump(xgb_model, os.path.join(Config.out_dir, 'xgb_user_model.joblib'))
    joblib.dump(scaler, os.path.join(Config.out_dir, 'xgb_user_scaler.joblib'))

    return train_merged, test_merged, xgb_model, scaler


# =======================
# Graph construction (HeteroData)
# =======================

def build_fusion_graph(df, bert_embeddings):
    print('\n--- Building Fusion Graph ---')

    # Embeddings must align with rows of df
    if bert_embeddings.shape[0] != len(df):
        raise ValueError('Number of embeddings must equal number of rows in dataframe')

    u_unique = df['user_id'].unique()
    b_unique = df['gmap_id'].unique()

    user_map = {uid: i for i, uid in enumerate(u_unique)}
    biz_map = {bid: i for i, bid in enumerate(b_unique)}

    # User node features
    user_agg = df.groupby('user_id').agg({
        'xgb_prob': 'mean',
        'rating': 'mean'
    }).reindex(u_unique).fillna(0)

    user_x = torch.tensor(user_agg[['rating', 'xgb_prob']].values, dtype=torch.float)

    # Business nodes
    biz_agg = df.groupby('gmap_id')['rating'].mean().reindex(b_unique).fillna(0)
    biz_x = torch.tensor(biz_agg.values.reshape(-1, 1), dtype=torch.float)

    # Edge index user->business
    src = [user_map[u] for u in df['user_id']]
    dst = [biz_map[b] for b in df['gmap_id']]
    edge_index = torch.tensor([src, dst], dtype=torch.long)

    edge_attr = torch.tensor(bert_embeddings, dtype=torch.float)
    y = torch.tensor(df[Config.LABEL_COL].values, dtype=torch.float)

    data = HeteroData()
    data['user'].x = user_x
    data['business'].x = biz_x
    data['user', 'reviews', 'business'].edge_index = edge_index
    data['user', 'reviews', 'business'].edge_attr = edge_attr
    data['user', 'reviews', 'business'].y = y
    data['business', 'rev_by', 'user'].edge_index = torch.tensor([dst, src], dtype=torch.long)
    data['business', 'rev_by', 'user'].edge_attr = edge_attr

    return data


# =======================
# Unified R-GCN model
# =======================
class UnifiedModel(nn.Module):
    def __init__(self, user_dim, biz_dim, edge_dim, hidden_dim=64):
        super().__init__()
        self.user_proj = nn.Linear(user_dim, hidden_dim)
        self.biz_proj = nn.Linear(biz_dim, hidden_dim)
        # two relation types: user->biz and biz->user
        self.rgcn1 = RGCNConv(hidden_dim, hidden_dim, num_relations=2)
        self.rgcn2 = RGCNConv(hidden_dim, hidden_dim, num_relations=2)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2 + edge_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x_dict, edge_index_dict, edge_attr_dict):
        u_h = F.relu(self.user_proj(x_dict['user']))
        b_h = F.relu(self.biz_proj(x_dict['business']))
        x = torch.cat([u_h, b_h], dim=0)
        u_nodes = u_h.size(0)

        idx1 = edge_index_dict[('user', 'reviews', 'business')]
        idx2 = edge_index_dict[('business', 'rev_by', 'user')]
        idx1_offset = torch.stack([idx1[0], idx1[1] + u_nodes])
        idx2_offset = torch.stack([idx2[0] + u_nodes, idx2[1]])
        full_edge_index = torch.cat([idx1_offset, idx2_offset], dim=1)

        edge_type = torch.cat([
            torch.zeros(idx1.size(1), dtype=torch.long, device=x.device),
            torch.ones(idx2.size(1), dtype=torch.long, device=x.device)
        ])

        h = self.rgcn1(x, full_edge_index, edge_type)
        h = F.relu(h)
        h = F.dropout(h, p=0.5, training=self.training)
        h = self.rgcn2(h, full_edge_index, edge_type)

        u_final = h[:u_nodes]
        b_final = h[u_nodes:]

        src_idx = idx1[0]
        dst_idx = idx1[1]
        u_embeds = u_final[src_idx]
        b_embeds = b_final[dst_idx]
        bert_embeds = edge_attr_dict[('user', 'reviews', 'business')]

        fusion_vec = torch.cat([u_embeds, b_embeds, bert_embeds], dim=1)
        return self.classifier(fusion_vec).squeeze()


# =======================
# Utilities: plotting
# =======================

def plot_roc(y_true, probs, fname=None):
    fpr, tpr, _ = roc_curve(y_true, probs)
    auc = roc_auc_score(y_true, probs)
    plt.figure()
    plt.plot(fpr, tpr)
    plt.plot([0, 1], [0, 1], linestyle='--', alpha=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve (AUC = {auc:.4f})')
    if fname:
        plt.savefig(fname)
    plt.close()


def plot_confusion(y_true, y_pred, fname=None):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure()
    plt.imshow(cm, interpolation='nearest')
    plt.title('Confusion Matrix')
    plt.colorbar()
    plt.xticks([0, 1])
    plt.yticks([0, 1])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    for (i, j), val in np.ndenumerate(cm):
        plt.text(j, i, int(val), ha='center', va='center')
    if fname:
        plt.savefig(fname)
    plt.close()


# =======================
# Prediction helper for single review
# =======================

def compute_bert_embedding_for_text(text):
    """Try to compute embedding for an incoming text using transformers locally.
    If transformers/models are not available this will raise.
    """
    print('Attempting to compute BERT embedding for input text...')
    tokenizer = AutoTokenizer.from_pretrained(Config.bert_model)
    model = AutoModel.from_pretrained(Config.bert_model).to(Config.device)
    model.eval()
    with torch.no_grad():
        enc = tokenizer([text], truncation=True, padding=True, max_length=128, return_tensors='pt').to(Config.device)
        out = model(**enc)
        emb = out.last_hidden_state[:, 0, :].cpu().numpy()
    return emb


class InferencePipeline:
    def __init__(self, model_path=None):
        # Load artifacts if available
        self.device = Config.device
        self.model = None
        self.xgb_model = None
        self.scaler = None

        if os.path.exists(os.path.join(Config.out_dir, 'xgb_user_model.joblib')):
            self.xgb_model = joblib.load(os.path.join(Config.out_dir, 'xgb_user_model.joblib'))
        if os.path.exists(os.path.join(Config.out_dir, 'xgb_user_scaler.joblib')):
            self.scaler = joblib.load(os.path.join(Config.out_dir, 'xgb_user_scaler.joblib'))
        if model_path and os.path.exists(model_path):
            self.model = torch.load(model_path, map_location=self.device)
            self.model.to(self.device)

    def predict_review(self, review_text, user_row, biz_row, compute_bert_for_input=True):
        """
        review_text: raw text
        user_row: dict-like with keys ['user_id','rating_mean','rating_std','review_count','biz_diversity'] OR a pandas Series
        biz_row: dict-like with key 'rating' (avg business rating)
        compute_bert_for_input: if True, attempt to compute BERT embedding locally
        Returns: dict with prob, pred, xgb_prob
        """
        # 1. compute or require embedding
        if compute_bert_for_input:
            try:
                emb = compute_bert_embedding_for_text(review_text)
            except Exception as e:
                raise RuntimeError('Failed to compute BERT embedding locally: ' + str(e))
        else:
            raise ValueError('This function requires an embedding or compute_bert_for_input=True')

        # 2. xgboost score
        feat_cols = ['rating_mean', 'rating_std', 'review_count', 'biz_diversity']
        user_vec = np.array([user_row.get(c, 0) for c in feat_cols]).reshape(1, -1)
        if self.scaler is None:
            raise RuntimeError('Scaler not found. Run training pipeline first to produce scaler.')
        user_vec_s = self.scaler.transform(user_vec)
        xgb_prob = float(self.xgb_model.predict_proba(user_vec_s)[:, 1])

        # 3. Build a tiny graph with single user and single biz and run model if loaded
        if self.model is None:
            return {'xgb_prob': xgb_prob, 'prob': xgb_prob, 'pred': int(xgb_prob > 0.5)}

        # build HeteroData
        data = HeteroData()
        data['user'].x = torch.tensor(np.array([[biz_row.get('rating', 0), xgb_prob]]), dtype=torch.float)
        data['business'].x = torch.tensor(np.array([[biz_row.get('rating', 0)]]), dtype=torch.float)

        # single edge
        data['user', 'reviews', 'business'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)
        data['user', 'reviews', 'business'].edge_attr = torch.tensor(emb, dtype=torch.float)
        data['user', 'reviews', 'business'].y = torch.tensor([0.0])
        data['business', 'rev_by', 'user'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)
        data['business', 'rev_by', 'user'].edge_attr = torch.tensor(emb, dtype=torch.float)

        data = data.to(self.device)
        self.model.eval()
        with torch.no_grad():
            logits = self.model(data.x_dict, data.edge_index_dict, data.edge_attr_dict)
            prob = torch.sigmoid(logits).cpu().numpy()[0]
            pred = int(prob > 0.5)

        return {'xgb_prob': xgb_prob, 'prob': float(prob), 'pred': pred}


# =======================
# Main training pipeline
# =======================

def main():
    train_df, test_df = load_and_prep_data()

    # Load embeddings (prefer existing .npy files)
    train_emb = load_embeddings(Config.TRAIN_EMB)
    test_emb = load_embeddings(Config.TEST_EMB)

    # XGBoost user behavior
    train_df, test_df, xgb_model, scaler = process_xgb_features(train_df, test_df)

    # Build graphs
    train_data = build_fusion_graph(train_df, train_emb)
    test_data = build_fusion_graph(test_df, test_emb)

    # Model init
    model = UnifiedModel(
        user_dim=train_data['user'].x.size(1),
        biz_dim=train_data['business'].x.size(1),
        edge_dim=train_emb.shape[1],
        hidden_dim=Config.rgcn_hidden_dim
    ).to(Config.device)

    optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)
    criterion = nn.BCEWithLogitsLoss()

    train_data = train_data.to(Config.device)
    test_data = test_data.to(Config.device)

    print('\n--- Starting Training ---')
    for epoch in range(Config.num_epochs):
        model.train()
        optimizer.zero_grad()
        out = model(train_data.x_dict, train_data.edge_index_dict, train_data.edge_attr_dict)
        loss = criterion(out, train_data['user', 'reviews', 'business'].y)
        loss.backward()
        optimizer.step()
        if epoch % 5 == 0 or epoch == Config.num_epochs - 1:
            print(f"Epoch {epoch} | Loss: {loss.item():.4f}")

    # Save model and artifacts
    model_path = os.path.join(Config.out_dir, 'rgcn_unified_model.pt')
    torch.save(model, model_path)
    joblib.dump(xgb_model, os.path.join(Config.out_dir, 'xgb_user_model.joblib'))
    joblib.dump(scaler, os.path.join(Config.out_dir, 'xgb_user_scaler.joblib'))
    print(f"Saved artifacts to {Config.out_dir}")

    # Evaluation
    print('\nFINAL EVALUATION')
    model.eval()
    with torch.no_grad():
        logits = model(test_data.x_dict, test_data.edge_index_dict, test_data.edge_attr_dict)
        probs = torch.sigmoid(logits).cpu().numpy()
        preds = (probs > 0.5).astype(int)
        y_true = test_data['user', 'reviews', 'business'].y.cpu().numpy()

    print(f"Accuracy: {accuracy_score(y_true, preds):.4f}")
    print(f"F1 Score: {f1_score(y_true, preds):.4f}")
    try:
        print(f"ROC-AUC:  {roc_auc_score(y_true, probs):.4f}")
    except Exception:
        print('ROC-AUC: could not compute (single-class?).')

    # Plot ROC and Confusion
    plot_roc(y_true, probs, fname=os.path.join(Config.out_dir, 'roc_curve.png'))
    plot_confusion(y_true, preds, fname=os.path.join(Config.out_dir, 'confusion_matrix.png'))

    # Random sample inspection
    print('\n' + '='*60)
    print('RANDOM SAMPLE INSPECTION (5 Random Reviews)')
    print('='*60)

    num_samples = min(5, len(test_df))
    random_indices = random.sample(range(len(test_df)), num_samples)

    for i, idx in enumerate(random_indices):
        print(f"\n[Sample #{i+1} - Index {idx}]")
        row = test_df.iloc[idx]
        text = row[Config.TEXT_COL]
        display_text = (text[:200] + '...') if len(text) > 200 else text
        true_label = 'FAKE (1)' if row[Config.LABEL_COL] == 1 else 'REAL (0)'
        xgb_score = row['xgb_prob']
        final_prob = float(probs[idx])
        final_pred = 'FAKE' if final_prob > 0.5 else 'REAL'
        confidence = final_prob if final_prob > 0.5 else (1 - final_prob)

        print(f'Review Text: "{display_text}"')
        print(f'True Label: {true_label}')
        print(f'XGBoost Suspicion Score: {xgb_score:.4f} (Behavioral)')
        print(f'Unified Model Prob: {final_prob:.4f}')
        print(f'FINAL PREDICTION: {final_pred} (Confidence: {confidence*100:.1f}%)')
        match = '✅ CORRECT' if (final_pred == 'FAKE' and row[Config.LABEL_COL] == 1) or \
                    (final_pred == 'REAL' and row[Config.LABEL_COL] == 0) else '❌ WRONG'
        print(f'Result: {match}')
        print('-'*40)

    print('\nDone. Artifacts and plots saved into', Config.out_dir)


if __name__ == '__main__':
    main()
