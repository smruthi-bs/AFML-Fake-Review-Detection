{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVvmv7FvJcm5",
        "outputId": "ea363d17-7dbf-469c-8009-1671a2564f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L851Ywsm_EL3",
        "outputId": "654828a8-219d-47e6-ddb3-8ea78b6f9dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Stage 1: Stratified User Selection\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-Kentucky_10.json.gz...\n",
            "  Found 145516 unique users in Kentucky.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-Hawaii_10.json.gz...\n",
            "  Found 64336 unique users in Hawaii.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-Minnesota_10.json.gz...\n",
            "  Found 182652 unique users in Minnesota.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-California_10.json.gz...\n",
            "  Found 1378206 unique users in California.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-Texas_10.json.gz...\n",
            "  Found 1260383 unique users in Texas.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-New_York_10.json.gz...\n",
            "  Found 630637 unique users in NewYork.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-Florida_10.json.gz...\n",
            "  Found 1155514 unique users in Florida.\n",
            "Loading users from: /content/drive/MyDrive/AFML_data/review-Illinois_10.json.gz...\n",
            "  Found 431906 unique users in Illinois.\n",
            "\n",
            "--- Stage 1 Complete ---\n",
            "Total unique TRAIN users selected: 159665\n",
            "Total unique TEST users selected: 39980\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "# Step 1: Define Strata (States) and Sampling Parameters\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/AFML_data/'\n",
        "\n",
        "# 1. Define your 8 states and their full file paths\n",
        "review_files_map = {\n",
        "    DATA_PATH + 'review-Kentucky_10.json.gz': 'Kentucky',\n",
        "    DATA_PATH + 'review-Hawaii_10.json.gz': 'Hawaii',\n",
        "    DATA_PATH + 'review-Minnesota_10.json.gz': 'Minnesota',\n",
        "    DATA_PATH + 'review-California_10.json.gz': 'California',\n",
        "    DATA_PATH + 'review-Texas_10.json.gz': 'Texas',\n",
        "    DATA_PATH + 'review-New_York_10.json.gz': 'NewYork',\n",
        "    DATA_PATH + 'review-Florida_10.json.gz': 'Florida',\n",
        "    DATA_PATH + 'review-Illinois_10.json.gz': 'Illinois'\n",
        "}\n",
        "\n",
        "# 2. INCREASED SAMPLING TARGETS\n",
        "# We need to start with MANY users because most will be filtered out\n",
        "# when we look for \"Travelers\" and \"Popular Businesses\".\n",
        "TARGET_TRAIN_USERS_PER_STATE = 20000\n",
        "TARGET_TEST_USERS_PER_STATE = 5000\n",
        "\n",
        "# --- THIS IS THE UNZIPPING CODE ---\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'r')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "# ----------------------------------\n",
        "\n",
        "# Step 2: Perform Stratified User Sampling\n",
        "\n",
        "print(\"Starting Stage 1: Stratified User Selection\")\n",
        "train_user_set = set()\n",
        "test_user_set = set()\n",
        "state_user_map = {}\n",
        "\n",
        "# 1. Loop 1: Find all users in each state (stratum)\n",
        "for file_path, state_name in review_files_map.items():\n",
        "    print(f\"Loading users from: {file_path}...\")\n",
        "    user_set_for_state = set()\n",
        "    try:\n",
        "        for review in parse(file_path):\n",
        "            if 'user_id' in review:\n",
        "                user_set_for_state.add(review['user_id'])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  WARNING: File not found: {file_path}. Skipping stratum.\")\n",
        "        continue\n",
        "\n",
        "    state_user_map[state_name] = user_set_for_state\n",
        "    print(f\"  Found {len(user_set_for_state)} unique users in {state_name}.\")\n",
        "\n",
        "# 2. Loop 2: Sample disjoint train/test sets from each stratum\n",
        "np.random.seed(42)\n",
        "for state_name, user_set in state_user_map.items():\n",
        "\n",
        "    user_list = list(user_set)\n",
        "    np.random.shuffle(user_list)\n",
        "\n",
        "    total_users_in_state = len(user_list)\n",
        "\n",
        "    # 1. Take Train Users\n",
        "    train_sample_size = min(TARGET_TRAIN_USERS_PER_STATE, total_users_in_state)\n",
        "    sampled_train_users = user_list[:train_sample_size]\n",
        "    train_user_set.update(sampled_train_users)\n",
        "\n",
        "    # 2. Take Test Users from the *remainder*\n",
        "    remaining_users = user_list[train_sample_size:]\n",
        "    test_sample_size = min(TARGET_TEST_USERS_PER_STATE, len(remaining_users))\n",
        "\n",
        "    if test_sample_size > 0:\n",
        "        sampled_test_users = remaining_users[:test_sample_size]\n",
        "        test_user_set.update(sampled_test_users)\n",
        "\n",
        "print(\"\\n--- Stage 1 Complete ---\")\n",
        "print(f\"Total unique TRAIN users selected: {len(train_user_set)}\")\n",
        "print(f\"Total unique TEST users selected: {len(test_user_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f--c8Lmn_Mi8",
        "outputId": "1fc9e69f-daa8-413a-8aa3-2899e72ed843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Stage 2: Loading full review data for selected users...\n",
            "Parsing /content/drive/MyDrive/AFML_data/review-Kentucky_10.json.gz for selected user data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter # Import Counter\n",
        "\n",
        "print(\"Starting Stage 2: Loading full review data for selected users...\")\n",
        "train_reviews_list = []\n",
        "test_reviews_list = []\n",
        "\n",
        "# Loop through all 8 files again to get full review data\n",
        "for file_path, state_name in review_files_map.items():\n",
        "    print(f\"Parsing {file_path} for selected user data...\")\n",
        "    try:\n",
        "        for review in parse(file_path):\n",
        "            user_id = review.get('user_id')\n",
        "\n",
        "            if user_id in train_user_set:\n",
        "                review['state'] = state_name\n",
        "                train_reviews_list.append(review)\n",
        "            elif user_id in test_user_set:\n",
        "                review['state'] = state_name\n",
        "                test_reviews_list.append(review)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  WARNING: File not found: {file_path}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "print(\"\\nFinished parsing all files. Now filtering for popular businesses...\")\n",
        "\n",
        "# --- STEP 1: Filter for gmap_ids with > 30 reviews ---\n",
        "MIN_REVIEWS_PER_BUSINESS = 30\n",
        "\n",
        "all_reviews_list = train_reviews_list + test_reviews_list\n",
        "print(f\"Total reviews found (Train+Test): {len(all_reviews_list)}\")\n",
        "\n",
        "all_gmap_ids = [r['gmap_id'] for r in all_reviews_list if 'gmap_id' in r]\n",
        "gmap_counts = Counter(all_gmap_ids)\n",
        "print(f\"Found {len(gmap_counts)} total unique businesses (gmap_ids).\")\n",
        "\n",
        "valid_gmap_ids = {gmap_id for gmap_id, count in gmap_counts.items() if count > MIN_REVIEWS_PER_BUSINESS}\n",
        "print(f\"Found {len(valid_gmap_ids)} businesses with > {MIN_REVIEWS_PER_BUSINESS} reviews.\")\n",
        "\n",
        "filtered_train_reviews_list = [r for r in train_reviews_list if r.get('gmap_id') in valid_gmap_ids]\n",
        "filtered_test_reviews_list = [r for r in test_reviews_list if r.get('gmap_id') in valid_gmap_ids]\n",
        "\n",
        "print(f\"Train reviews reduced from {len(train_reviews_list)} to {len(filtered_train_reviews_list)}\")\n",
        "print(f\"Test reviews reduced from {len(test_reviews_list)} to {len(filtered_test_reviews_list)}\")\n",
        "# --- End of STEP 1 ---\n",
        "\n",
        "\n",
        "print(\"\\nNow processing filtered datasets...\")\n",
        "\n",
        "def process_and_save_dataset(reviews_list, output_csv_name, id_offset=0):\n",
        "    \"\"\"\n",
        "    Processes a list of reviews, applies gmap_id filter, then\n",
        "    filters for users with 'is_diff_state=True', saves to CSV,\n",
        "    and returns the final DataFrame AND the max ID used before user filtering.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Processing dataset for {output_csv_name} ---\")\n",
        "    if not reviews_list:\n",
        "        print(\"No reviews found for this dataset. Skipping.\")\n",
        "        return None, id_offset # Return offset unchanged\n",
        "\n",
        "    # Step 3: Create DataFrame and Clean\n",
        "    print(\"Creating final DataFrame...\")\n",
        "    all_reviews_df = pd.DataFrame(reviews_list)\n",
        "\n",
        "    columns_to_keep = ['user_id', 'rating', 'text', 'gmap_id', 'state', 'time']\n",
        "    valid_columns = [col for col in columns_to_keep if col in all_reviews_df.columns]\n",
        "    all_reviews_df = all_reviews_df[valid_columns]\n",
        "\n",
        "    print(\"Cleaning data and converting datetime...\")\n",
        "    cleaned_df = all_reviews_df.dropna(subset=['user_id', 'text', 'rating', 'gmap_id'])\n",
        "\n",
        "    cleaned_df = cleaned_df.copy()\n",
        "    cleaned_df['datetime'] = pd.to_datetime(cleaned_df['time'], unit='ms')\n",
        "    cleaned_df = cleaned_df.drop(columns=['time'])\n",
        "\n",
        "    # Step 4: Create Timeline & Geodiversity Features\n",
        "    print(\"\\nSorting DataFrame by user and time...\")\n",
        "    df_sorted = cleaned_df.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "    print(\"Calculating time and state differences...\")\n",
        "    df_sorted['last_review_datetime'] = df_sorted.groupby('user_id')['datetime'].shift()\n",
        "    df_sorted['last_review_state'] = df_sorted.groupby('user_id')['state'].shift()\n",
        "\n",
        "    df_sorted['time_since_last_review_sec'] = (df_sorted['datetime'] - df_sorted['last_review_datetime']).dt.total_seconds()\n",
        "    df_sorted['is_diff_state'] = (df_sorted['state'] != df_sorted['last_review_state'])\n",
        "\n",
        "    df_sorted['time_since_last_review_sec'] = df_sorted['time_since_last_review_sec'].fillna(-1)\n",
        "    df_sorted.loc[df_sorted['last_review_state'].isnull(), 'is_diff_state'] = False\n",
        "\n",
        "    print(\"Creating final review_id column...\")\n",
        "    df_sorted.reset_index(drop=True, inplace=True)\n",
        "    df_sorted['review_id'] = df_sorted.index + 1 + id_offset\n",
        "\n",
        "    # Capture the max ID before we filter users\n",
        "    rows_before_user_filter = len(df_sorted)\n",
        "\n",
        "    # --- NEW STEP 2: Filter for users with at least one 'is_diff_state=True' ---\n",
        "    print(\"\\n--- Applying 'is_diff_state' user filter ---\")\n",
        "    true_count_before = df_sorted['is_diff_state'].sum()\n",
        "    total_reviews_before = len(df_sorted)\n",
        "    if total_reviews_before > 0:\n",
        "        print(f\"  Before filter: {true_count_before} 'True' reviews out of {total_reviews_before} total ({true_count_before/total_reviews_before:.2%})\")\n",
        "    else:\n",
        "        print(\"  Before filter: 0 reviews.\")\n",
        "\n",
        "    users_with_diff_state_true = df_sorted[df_sorted['is_diff_state'] == True]['user_id'].unique()\n",
        "    print(f\"  Found {len(users_with_diff_state_true)} users with at least one 'is_diff_state=True' review.\")\n",
        "\n",
        "    if len(users_with_diff_state_true) == 0:\n",
        "        print(\"  WARNING: No users found with 'is_diff_state=True'. The resulting file will be empty.\")\n",
        "        # Create an empty dataframe with the correct columns to avoid errors\n",
        "        df_final_filtered = pd.DataFrame(columns=df_sorted.columns)\n",
        "    else:\n",
        "        print(f\"  Filtering DataFrame to *only* include all reviews from these users.\")\n",
        "        df_final_filtered = df_sorted[df_sorted['user_id'].isin(users_with_diff_state_true)]\n",
        "\n",
        "    true_count_after = df_final_filtered['is_diff_state'].sum()\n",
        "    total_reviews_after = len(df_final_filtered)\n",
        "\n",
        "    if total_reviews_after > 0:\n",
        "        print(f\"  After filter: {total_reviews_after} total reviews remaining.\")\n",
        "        print(f\"  New balance: {true_count_after} 'True' reviews ({true_count_after/total_reviews_after:.2%})\")\n",
        "    else:\n",
        "        print(\"  After filter: 0 total reviews remaining.\")\n",
        "    # --- End of NEW STEP 2 ---\n",
        "\n",
        "    # Step 5: Save Your Final Enriched CSV\n",
        "    final_columns_to_save = [\n",
        "        'review_id',\n",
        "        'user_id',\n",
        "        'rating',\n",
        "        'text',\n",
        "        'gmap_id',\n",
        "        'state', # <-- ADDED THIS COLUMN\n",
        "        'datetime',\n",
        "        'last_review_datetime',\n",
        "        'time_since_last_review_sec',\n",
        "        'is_diff_state'\n",
        "    ]\n",
        "    final_columns_to_save = [col for col in final_columns_to_save if col in df_final_filtered.columns]\n",
        "\n",
        "    print(f\"\\nFinal columns will be: {final_columns_to_save}\")\n",
        "    # Use the filtered dataframe\n",
        "    final_df_to_save = df_final_filtered[final_columns_to_save]\n",
        "\n",
        "    # --- MODIFIED: New file name based on your targets ---\n",
        "    final_csv_path = output_csv_name # Use the name directly\n",
        "    print(f\"\\nSaving final stratified data to {final_csv_path}...\")\n",
        "    final_df_to_save.to_csv(final_csv_path, index=False)\n",
        "\n",
        "    print(f\"--- Processing Complete! ---\")\n",
        "    print(f\"Your file '{final_csv_path}' is ready. (Total Reviews: {len(final_df_to_save)})\\\\n\")\n",
        "\n",
        "    # Return the filtered DF and the count *before* this filter for the offset\n",
        "    return final_df_to_save, rows_before_user_filter\n",
        "\n",
        "# --- End of function definition ---\n",
        "\n",
        "# Now, process both FILTERED datasets sequentially\n",
        "\n",
        "# 1. Process train first, starting with ID 1 (offset=0)\n",
        "# *** UPDATED FILENAMES ***\n",
        "train_df, last_train_id = process_and_save_dataset(\n",
        "    filtered_train_reviews_list,\n",
        "    'TRAIN_filtered_target_100k.csv',\n",
        "    id_offset=0\n",
        ")\n",
        "\n",
        "# 2. Get the max ID from the train set *before* the user filter\n",
        "if train_df is None:\n",
        "    last_train_id = 0 # Handle case where train set might be empty\n",
        "\n",
        "print(f\"Train set complete. Max review_id assigned was: {last_train_id}\")\n",
        "\n",
        "# 3. Process test, starting IDs *after* the last train ID\n",
        "print(f\"Starting test set processing with review_id offset of: {last_train_id}\")\n",
        "# *** UPDATED FILENAMES ***\n",
        "test_df, _ = process_and_save_dataset(\n",
        "    filtered_test_reviews_list,\n",
        "    'TEST_filtered_target_25k.csv',\n",
        "    id_offset=last_train_id\n",
        ")\n",
        "\n",
        "print(\"--- All Done! Both filtered files have been created with globally unique review_ids. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. SETTINGS ---\n",
        "INPUT_FILE = 'TRAIN_final_100k_clustered.csv'\n",
        "OUTPUT_FILE = 'TRAIN_model_ready_final_mode.csv'\n",
        "print(f\"Loading {INPUT_FILE}...\")\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE, parse_dates=['datetime', 'last_review_datetime'])\n",
        "print(f\"Loaded {len(df)} reviews.\")\n",
        "\n",
        "# --- 2. Feature Engineering: Datetime ---\n",
        "print(\"Extracting datetime features...\")\n",
        "df['review_hour'] = df['datetime'].dt.hour\n",
        "df['review_age_days'] = (df['datetime'].max() - df['datetime']).dt.total_seconds() / (60*60*24)\n",
        "\n",
        "# --- 3. Bucketing ---\n",
        "print(\"Bucketing 'time_since_last_review_sec'...\")\n",
        "bins = [-np.inf, -1, 0, 14400, 86400, 604800, 2592000, np.inf]\n",
        "labels = ['First_Review', 'Same_Time', 'Under_4_Hours', 'Under_1_Day', 'Under_1_Week', 'Under_30_Days', 'Over_30_Days']\n",
        "df['time_since_last_bucket'] = pd.cut(df['time_since_last_review_sec'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# --- 4. User Behavior Features (MODE) ---\n",
        "print(\"Calculating user behavior features...\")\n",
        "def get_mode(x):\n",
        "    if x.mode().empty: return np.nan\n",
        "    return x.mode().iloc[0]\n",
        "\n",
        "user_grouped = df.groupby('user_id')\n",
        "user_stats = user_grouped.agg(\n",
        "    user_review_count=('review_id', 'count'),\n",
        "    user_mode_rating=('rating', get_mode),\n",
        "    user_rating_variance=('rating', 'var'),\n",
        "    user_gmap_diversity=('gmap_id', 'nunique'),\n",
        "    user_state_diversity=('state', 'nunique')\n",
        ").reset_index()\n",
        "\n",
        "user_stats['user_rating_variance'] = user_stats['user_rating_variance'].fillna(0)\n",
        "df = pd.merge(df, user_stats, on='user_id', how='left')\n",
        "\n",
        "# --- 5. Business Features (MODE) ---\n",
        "print(\"Calculating business features...\")\n",
        "business_grouped = df.groupby('gmap_id')\n",
        "business_stats = business_grouped.agg(\n",
        "    business_review_count=('review_id', 'count'),\n",
        "    business_mode_rating=('rating', get_mode),\n",
        "    business_rating_variance=('rating', 'var')\n",
        ").reset_index()\n",
        "\n",
        "business_stats['business_rating_variance'] = business_stats['business_rating_variance'].fillna(0)\n",
        "df = pd.merge(df, business_stats, on='gmap_id', how='left')\n",
        "\n",
        "# --- 6. Save ---\n",
        "\n",
        "print(f\"Saving to {OUTPUT_FILE}...\")\n",
        "# Drop raw columns we don't need for the model\n",
        "final_df = df.drop(columns=['time_since_last_review_sec', 'last_review_datetime'])\n",
        "final_df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"--- Complete! Saved {OUTPUT_FILE} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os5UDV-DAoAY",
        "outputId": "87d3d8e1-9bb5-469d-c964-68d8a1081aa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TRAIN_final_100k_clustered.csv...\n",
            "Loaded 100008 reviews.\n",
            "Extracting datetime features...\n",
            "Bucketing 'time_since_last_review_sec'...\n",
            "Calculating user behavior features...\n",
            "Calculating business features...\n",
            "Saving to TRAIN_model_ready_final_mode.csv...\n",
            "--- Complete! Saved TRAIN_model_ready_final_mode.csv ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. SETTINGS ---\n",
        "INPUT_FILE = 'TEST_final_25k_clustered.csv'  # <-- Use the test file\n",
        "OUTPUT_FILE = 'TEST_model_ready_final_mode.csv' # <-- Create a new test file\n",
        "print(f\"Loading {INPUT_FILE}...\")\n",
        "\n",
        "# Load the dataset, making sure to parse datetime columns\n",
        "df = pd.read_csv(\n",
        "    INPUT_FILE,\n",
        "    parse_dates=['datetime', 'last_review_datetime']\n",
        ")\n",
        "print(f\"Loaded {len(df)} reviews.\")\n",
        "\n",
        "\n",
        "# --- 2. Feature Engineering: Datetime ---\n",
        "print(\"Extracting datetime features...\")\n",
        "df['review_hour'] = df['datetime'].dt.hour\n",
        "df['review_age_days'] = (df['datetime'].max() - df['datetime']).dt.total_seconds() / (60*60*24)\n",
        "\n",
        "\n",
        "# --- 3. Feature Engineering: Bucketing 'time_since_last_review_sec' ---\n",
        "print(\"Bucketing 'time_since_last_review_sec'...\")\n",
        "bins = [-np.inf, -1, 0, 14400, 86400, 604800, 2592000, np.inf]\n",
        "labels = [\n",
        "    'First_Review',\n",
        "    'Same_Time',\n",
        "    'Under_4_Hours',\n",
        "    'Under_1_Day',\n",
        "    'Under_1_Week',\n",
        "    'Under_30_Days',\n",
        "    'Over_30_Days'\n",
        "]\n",
        "df['time_since_last_bucket'] = pd.cut(df['time_since_last_review_sec'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "\n",
        "# --- 4. User Behavior Features (with MODE) ---\n",
        "print(\"Calculating user behavior features...\")\n",
        "\n",
        "# Define a function to safely get the first mode\n",
        "def get_mode(x):\n",
        "    if x.mode().empty:\n",
        "        return np.nan\n",
        "    return x.mode().iloc[0]\n",
        "\n",
        "user_grouped = df.groupby('user_id')\n",
        "\n",
        "user_stats = user_grouped.agg(\n",
        "    user_review_count=('review_id', 'count'),\n",
        "    user_mode_rating=('rating', get_mode), # <-- CHANGED TO MODE\n",
        "    user_rating_variance=('rating', 'var'),\n",
        "    user_gmap_diversity=('gmap_id', 'nunique'),\n",
        "    user_state_diversity=('state', 'nunique')\n",
        ").reset_index()\n",
        "\n",
        "user_stats['user_rating_variance'] = user_stats['user_rating_variance'].fillna(0)\n",
        "df = pd.merge(df, user_stats, on='user_id', how='left')\n",
        "\n",
        "\n",
        "# --- 5. Business Features (with MODE) ---\n",
        "print(\"Calculating business features...\")\n",
        "business_grouped = df.groupby('gmap_id')\n",
        "\n",
        "business_stats = business_grouped.agg(\n",
        "    business_review_count=('review_id', 'count'),\n",
        "    business_mode_rating=('rating', get_mode), # <-- CHANGED TO MODE\n",
        "    business_rating_variance=('rating', 'var')\n",
        ").reset_index()\n",
        "\n",
        "business_stats['business_rating_variance'] = business_stats['business_rating_variance'].fillna(0)\n",
        "df = pd.merge(df, business_stats, on='gmap_id', how='left')\n",
        "\n",
        "\n",
        "# --- 6. Save Final File ---\n",
        "print(f\"All features created. Cleaning up redundant columns...\")\n",
        "\n",
        "# Drop the raw columns that are no longer needed\n",
        "final_df = df.drop(columns=[\n",
        "    'time_since_last_review_sec', # Replaced by the bucket\n",
        "    'last_review_datetime'        # No longer needed by any model\n",
        "])\n",
        "\n",
        "print(f\"Saving to {OUTPUT_FILE}...\")\n",
        "final_df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(\"\\n--- Preprocessing Complete! ---\")\n",
        "print(f\"Your file '{OUTPUT_FILE}' is ready for modeling.\")\n",
        "print(\"\\nFinal Columns:\")\n",
        "print(final_df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgboxVM9njsL",
        "outputId": "238053a1-a451-4e3f-92d9-35011f46cc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TEST_final_25k_clustered.csv...\n",
            "Loaded 25002 reviews.\n",
            "Extracting datetime features...\n",
            "Bucketing 'time_since_last_review_sec'...\n",
            "Calculating user behavior features...\n",
            "Calculating business features...\n",
            "All features created. Cleaning up redundant columns...\n",
            "Saving to TEST_model_ready_final_mode.csv...\n",
            "\n",
            "--- Preprocessing Complete! ---\n",
            "Your file 'TEST_model_ready_final_mode.csv' is ready for modeling.\n",
            "\n",
            "Final Columns:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25002 entries, 0 to 25001\n",
            "Data columns (total 19 columns):\n",
            " #   Column                    Non-Null Count  Dtype         \n",
            "---  ------                    --------------  -----         \n",
            " 0   review_id                 25002 non-null  int64         \n",
            " 1   user_id                   25002 non-null  object        \n",
            " 2   rating                    25002 non-null  int64         \n",
            " 3   text                      25002 non-null  object        \n",
            " 4   gmap_id                   25002 non-null  object        \n",
            " 5   state                     25002 non-null  object        \n",
            " 6   datetime                  25002 non-null  datetime64[ns]\n",
            " 7   fast_travel               25002 non-null  bool          \n",
            " 8   review_hour               25002 non-null  int32         \n",
            " 9   review_age_days           25002 non-null  float64       \n",
            " 10  time_since_last_bucket    25002 non-null  category      \n",
            " 11  user_review_count         25002 non-null  int64         \n",
            " 12  user_mode_rating          25002 non-null  int64         \n",
            " 13  user_rating_variance      25002 non-null  float64       \n",
            " 14  user_gmap_diversity       25002 non-null  int64         \n",
            " 15  user_state_diversity      25002 non-null  int64         \n",
            " 16  business_review_count     25002 non-null  int64         \n",
            " 17  business_mode_rating      25002 non-null  int64         \n",
            " 18  business_rating_variance  25002 non-null  float64       \n",
            "dtypes: bool(1), category(1), datetime64[ns](1), float64(3), int32(1), int64(8), object(4)\n",
            "memory usage: 3.2+ MB\n",
            "None\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}